# -*- coding: utf-8 -*-
"""Netflix Stock Prediction Assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uoWv13cLmMuwngeYAOlpw0m2Z3f2P8Jq

1. Reading the Dataset
"""

# Importing required libraries
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset (ensure that the file name and path are correct)
df = pd.read_csv('NFLX.csv')

# Display the first 5 rows of the dataset
print(df.head())

# Essential Libraries
import pandas as pd
import numpy as np

# Reading the CSV file 'NFLX.csv' into a pandas DataFrame called 'Netflixdata'
Netflixdata = pd.read_csv('NFLX.csv')

# Printing the shape of the DataFrame before deleting duplicate values
# This outputs the number of rows and columns in the DataFrame
print('Table before deleting duplicate values:', Netflixdata.shape)

# Removing duplicate rows from the DataFrame and storing the result in 'output'
output = Netflixdata.drop_duplicates()

# Displaying the first 10 rows of the 'Netflixdata' DataFrame
Netflixdata.head(10)

"""2. Problem Statement Definition

The objective of this analysis is to develop a predictive model that can accurately anticipate the closing price of Netflix's (NFLX) stock based on past data. The closing price represents the final price at which the stock is traded on a specific day and is a critical indicator of the stock's performance.



---
The dataset used in this analysis is based on the Netflix stock price prediction Available from Kaggle


 https://www.kaggle.com/datasets/jainilcoder/netflix-stock-price-prediction/data

3. Target Variable Identification

The target variable for this analysis is the close price.

4. Data exploration at basic level
"""

# Essential Libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Define the target variable (if not already done)
target = df['Close']

# Set the plot size for better readability
plt.figure(figsize=(10, 6))

# Use Seaborn to create a histogram with a density plot for the target variable
sns.histplot(target, kde=True, color='blue')

# Add title and labels for clarity
plt.title('Distribution of Netflix Closing Prices')
plt.xlabel('Closing Price')
plt.ylabel('Frequency')

# Show the plot
plt.show()

"""5. Data exploration at basic leve

5.1 Data Info
"""

# Printing summary information about the DataFrame 'data'
# This includes the number of non-null entries, data types of each column,
print(data.info())

"""5.2 Data Description"""

# Printing summary statistics for numerical columns in the DataFrame 'data'
# This includes count, mean, standard deviation, minimum, maximum, and quartiles (25%, 50%, 75%)
print(data.describe())

"""5.3 Table Number of Unique Value"""

# Reading the CSV file 'NFLX.csv' into a pandas DataFrame called 'data'
data = pd.read_csv('NFLX.csv')

# Getting the number of unique values for each column in the DataFrame 'data'
# This helps to understand the diversity of values in each column and identify columns with duplicate or constant values
data.nunique()

"""6. Identifying and Rejecting useless columns"""

# Import the Pandas library for data handling
import pandas as pd

# Load the Netflix dataset from a CSV file, specifying 'latin' encoding
netflix_data = pd.read_csv('NFLX.csv', encoding='latin')

# Display the list of columns present in the DataFrame
print("Columns in the dataset:", netflix_data.columns.tolist())

# Remove the 'Date' and 'Adj Close' columns from the DataFrame
modified_data = netflix_data.drop(columns=['Date', 'Adj Close'])

# Show the DataFrame after removing specified columns
print(modified_data)

"""7. Visual Exploratory Data Analysis of data with Histogram"""

# Create histograms for key features to understand their distributions

# Set up a 2x2 grid of plots
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot histogram for 'Open' prices
sns.histplot(df['Open'], kde=True, color='green', ax=axes[0, 0])
axes[0, 0].set_title('Distribution of Netflix Open Prices')

# Plot histogram for 'High' prices
sns.histplot(df['High'], kde=True, color='orange', ax=axes[0, 1])
axes[0, 1].set_title('Distribution of Netflix High Prices')

# Plot histogram for 'Low' prices
sns.histplot(df['Low'], kde=True, color='red', ax=axes[1, 0])
axes[1, 0].set_title('Distribution of Netflix Low Prices')

# Plot histogram for 'Volume' (number of shares traded)
sns.histplot(df['Volume'], kde=True, color='purple', ax=axes[1, 1])
axes[1, 1].set_title('Distribution of Netflix Trading Volume')

# Adjustment of the layout
plt.tight_layout()

# Show the plots
plt.show()

"""8. Feature Selection based on data distribution"""

import pandas as pd

# Assuming 'NFLX 3.csv' is in the correct location and contains the data
Netflixdata = pd.read_csv('NFLX.csv')

continuousCols = ['Open', 'High', 'Low', 'Close', 'Volume']

# Use 'corr' instead of 'carr' to calculate the correlation matrix
CorrelationData = Netflixdata[continuousCols].corr()
print(CorrelationData)

"""8.1 Correlation Data"""

# Sorting the values in the 'High' column of the DataFrame 'CorrelationData' in descending order
# This helps to view the highest values in the 'High' column at the top of the list
# 'ascending=False' ensures that the sorting is done from highest to lowest
CorrelationData['High'].sort_values(ascending=False)

"""9. Removal of outliers and missing values

9.1 Libraries
"""

# Essential Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

"""9.2 Load Data"""

# Load the data from a CSV file into a DataFrame
data = pd.read_csv('NFLX.csv')

"""9.3 Data Inspection"""

# Set up the visualization style for plots
sns.set(style="whitegrid")

# Take a look at the initial information about the data
print("Basic information about the dataset:")
print(data.info())

# Check for any missing values in the dataset
print("\nMissing values in the data:")
print(data.isnull().sum())

"""9.4 Handle Missing Values"""

# Handle missing values:
# Convert columns to numeric, coercing any errors to NaN
numeric_columns = ['Close', 'Open', 'High', 'Low', 'Volume']  # Adjust column names as needed
for col in numeric_columns:
    data[col] = pd.to_numeric(data[col], errors='coerce')

# Drop any rows that still have NaN values after conversion
data = data.dropna()

"""9.5 Outlier Detection"""

# Detect outliers using the Interquartile Range (IQR) method
# Calculate the first (Q1) and third (Q3) quartiles
Q1 = data[numeric_columns].quantile(0.25)
Q3 = data[numeric_columns].quantile(0.75)
IQR = Q3 - Q1

# Filter out rows where any column value is an outlier
data_no_outliers_iqr = data[~((data[numeric_columns] < (Q1 - 1.5 * IQR)) | (data[numeric_columns] > (Q3 + 1.5 * IQR))).any(axis=1)]

# Detect outliers using the Z-score method
# Compute Z-scores for each value
z_scores = stats.zscore(data[numeric_columns])
abs_z_scores = np.abs(z_scores)
threshold = 3  # Define the Z-score threshold
data_no_outliers_z = data[(abs_z_scores < threshold).all(axis=1)]

"""9.6 Visualisation"""

# Plotting the data to visualize outliers and relationships
plt.figure(figsize=(14, 8))

# Create a box plot for the 'Close' column to visualize outliers
plt.subplot(1, 2, 1)
sns.boxplot(x=data['Close'])
plt.title('Boxplot of Closing Prices')

plt.tight_layout()
plt.show()

# Scatter plots to examine the relationships between continuous variables and 'Close'
ContinuousCols = ['Open', 'High', 'Low', 'Volume']
for predictor in ContinuousCols:
    data.plot.scatter(x=predictor, y='Close', figsize=(10,5), title=f'Scatter Plot of {predictor} vs. Close')
    plt.show()

"""10. Visual and Statistic Correlation analysis for selection of best features

10.1 Essential Libraries
"""

# Import essential libraries
import pandas as pd
from scipy.stats import f_oneway

"""10.2 Dataset"""

# Load the dataset
Netflixdata = pd.read_csv('NFLX.csv')

"""10.3 Results Analysis

"""

# Defines the function to analyse the relationship between categorical predictors and the target variable
def perform_results_analysis(data, target, predictors):

    # Evaluates whether each categorical predictor has a significant relationship with the target variable.
    significant_predictors = []

    print('Results of the Statistical Test \n')
    for predictor in predictors:
        # Group the data by the predictor and collect the target variable values for each group
        groups = data.groupby(predictor)[target].apply(list)
        # Perform the ANOVA test
        test_result = f_oneway(*groups)

        # Check if the p-value indicates a significant relationship
        if test_result.pvalue < 0.05:
            print(f'{predictor} is significantly related to {target} | P-Value: {test_result.pvalue:.4e}')
            significant_predictors.append(predictor)
        else:
            print(f'{predictor} is not significantly related to {target} | P-Value: {test_result.pvalue:.4e}')

    return significant_predictors

"""10.4 List of Categorical Predictors and Relationships"""

# List of categorical predictors to analyse
categorical_predictors = ['Open', 'Low']
# Find which predictors have a significant relationship with the target variable
significant_vars = perform_results_analysis(data=Netflixdata, target='Close', predictors=categorical_predictors)

"""10.5 Prepare Data for Machine Learning"""

# Define the columns for machine learning processing
selected_features = ['Open', 'High', 'Low', 'Close']

# Create a DataFrame with only the selected columns
ml_data = Netflixdata[selected_features]

# Save this subset of data for future reference
ml_data.to_pickle('ProcessedData.pkl')

"""10.6 Convert Categorical Variables and Display Data"""

# Convert categorical variables to numeric format using dummy variables
numeric_data = pd.get_dummies(Netflixdata[selected_features])
numeric_data['Close'] = Netflixdata['Close']

# Display the first few rows of the processed data
print("Here is the processed data:\n", numeric_data.head())

"""11 Data Conversion to numeric values for machine learning/predictive analysis

11.1 Essential Libraries
"""

# Import essential libraries
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

"""11.2 Simulated Dataset"""

# Simulate a dataset with 1009 samples and 3 feature columns
num_samples = 1009
num_features = 3

# Creating a DataFrame with three features and one target variable
# Features are generated to cover a range of numbers for simplicity
data = pd.DataFrame({
    'Feature1': range(num_samples),
    'Feature2': range(num_samples, 2*num_samples),
    'Feature3': range(2*num_samples, 3*num_samples),
    'Close': range(num_samples)  # Target variable
})

"""11.3 Convert Categorical Data to Numeric"""

# Convert categorical data to numeric form if there are any categorical features
df = pd.get_dummies(data, drop_first=True)

"""11.4 Define Feature Matrix and Target Vector"""

# Extract the target variable and feature columns
TargetVariable = 'Close'
Predictors = df.columns[df.columns != TargetVariable].tolist()

# Define feature matrix (X) and target vector (y)
X = df[Predictors].values
y = df[TargetVariable].values

"""11.5 Split the Dataset into Training and Testing Sets"""

# Split the dataset into training and testing sets
# 70% of data for training, and 30% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""11.6 Scale the Features and Output Data"""

# Initialize the MinMaxScaler to scale features to a range of [0, 1]
scaler = MinMaxScaler()

# Fit the scaler on the training data and transform both training and testing datasets
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Output the shapes of the resulting datasets to ensure they match expectations
print("Training feature set:", X_train_scaled.shape)  # Expected: (706, 3)
print("Training target set:", y_train.shape)         # Expected: (706,)
print("Testing feature set:", X_test_scaled.shape)   # Expected: (303, 3)
print("Testing target set:", y_test.shape)           # Expected: (303,)